{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f3pyUCpwdV1"
      },
      "outputs": [],
      "source": [
        "\n",
        "import googleapiclient.discovery\n",
        "import pandas as pd\n",
        "\n",
        "api_service_name = \"youtube\"\n",
        "api_version = \"v3\"\n",
        "DEVELOPER_KEY = \"AIzaSyBXSKEf_KfmdYHvrLugt-KoJBSZyYXLG5A\"\n",
        "\n",
        "youtube = googleapiclient.discovery.build(\n",
        "    api_service_name, api_version, developerKey=DEVELOPER_KEY)\n",
        "\n",
        "request = youtube.commentThreads().list(\n",
        "    part=\"snippet\",\n",
        "    videoId=\"-GJgqIJsTME\",\n",
        "    maxResults=100\n",
        ")\n",
        "\n",
        "comments = []\n",
        "\n",
        "# Execute the request.\n",
        "response = request.execute()\n",
        "\n",
        "# Get the comments from the response.\n",
        "for item in response['items']:\n",
        "    comment = item['snippet']['topLevelComment']['snippet']\n",
        "    public = item['snippet']['isPublic']\n",
        "    comments.append([\n",
        "        comment['authorDisplayName'],\n",
        "        comment['publishedAt'],\n",
        "        comment['likeCount'],\n",
        "        comment['textOriginal'],\n",
        "        public\n",
        "    ])\n",
        "\n",
        "while (1 == 1):\n",
        "  try:\n",
        "   nextPageToken = response['nextPageToken']\n",
        "  except KeyError:\n",
        "   break\n",
        "  nextPageToken = response['nextPageToken']\n",
        "  # Create a new request object with the next page token.\n",
        "  nextRequest = youtube.commentThreads().list(part=\"snippet\", videoId=\"-GJgqIJsTME\", maxResults=100, pageToken=nextPageToken)\n",
        "  # Execute the next request.\n",
        "  response = nextRequest.execute()\n",
        "  # Get the comments from the next response.\n",
        "  for item in response['items']:\n",
        "    comment = item['snippet']['topLevelComment']['snippet']\n",
        "    public = item['snippet']['isPublic']\n",
        "    comments.append([\n",
        "        comment['authorDisplayName'],\n",
        "        comment['publishedAt'],\n",
        "        comment['likeCount'],\n",
        "        comment['textOriginal'],\n",
        "        public\n",
        "    ])\n",
        "\n",
        "df = pd.DataFrame(comments, columns=['author', 'updated_at', 'like_count', 'text','public'])\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymilvus\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw3KsobS0gWb",
        "outputId": "e435e865-67ab-4a13-dfbe-2639b740cd8a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymilvus in /usr/local/lib/python3.10/dist-packages (2.4.3)\n",
            "Requirement already satisfied: setuptools>=67 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (67.7.2)\n",
            "Requirement already satisfied: grpcio<=1.63.0,>=1.49.1 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (1.63.0)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (3.20.3)\n",
            "Requirement already satisfied: environs<=9.5.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (9.5.0)\n",
            "Requirement already satisfied: ujson>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (5.10.0)\n",
            "Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (2.0.3)\n",
            "Requirement already satisfied: milvus-lite<2.5.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (2.4.6)\n",
            "Requirement already satisfied: marshmallow>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from environs<=9.5.0->pymilvus) (3.21.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from environs<=9.5.0->pymilvus) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (1.25.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow>=3.0.0->environs<=9.5.0->pymilvus) (24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import googleapiclient.discovery\n",
        "import pandas as pd\n",
        "\n",
        "def extract_video_id(youtube_url):\n",
        "    \"\"\"\n",
        "    Extracts the video ID from a YouTube URL.\n",
        "\n",
        "    Args:\n",
        "        youtube_url (str): The YouTube video URL.\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted video ID.\n",
        "    \"\"\"\n",
        "    pattern = r\"(?<=v=)[a-zA-Z0-9_-]+(?=&|\\s|$)\"\n",
        "    match = re.search(pattern, youtube_url)\n",
        "    if match:\n",
        "        return match.group(0)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def get_video_comments(video_id, api_key):\n",
        "    \"\"\"\n",
        "    Retrieves comments from a YouTube video using the YouTube Data API v3.\n",
        "\n",
        "    Args:\n",
        "        video_id (str): The ID of the YouTube video.\n",
        "        api_key (str): The API key for accessing the YouTube Data API.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries containing comments data.\n",
        "    \"\"\"\n",
        "    youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=api_key)\n",
        "    comments = []\n",
        "    nextPageToken = None\n",
        "\n",
        "    while True:\n",
        "        request = youtube.commentThreads().list(\n",
        "            part=\"snippet\",\n",
        "            videoId=video_id,\n",
        "            maxResults=100,\n",
        "            pageToken=nextPageToken if nextPageToken else None\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            response = request.execute()\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            break\n",
        "\n",
        "        for item in response['items']:\n",
        "            comment = item['snippet']['topLevelComment']['snippet']\n",
        "            public = item['snippet']['isPublic']\n",
        "            comments.append({\n",
        "                \"author\": comment['authorDisplayName'],\n",
        "                \"published_at\": comment['publishedAt'],\n",
        "                \"like_count\": comment.get('likeCount', 0),  # Handle missing likeCount\n",
        "                \"text\": comment['textOriginal'],\n",
        "                \"public\": public\n",
        "            })\n",
        "\n",
        "        if 'nextPageToken' in response:\n",
        "            nextPageToken = response['nextPageToken']\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return comments\n",
        "\n",
        "def main():\n",
        "    youtube_url = input(\"Enter the YouTube video URL: \")\n",
        "    video_id = extract_video_id(youtube_url)\n",
        "\n",
        "    if video_id:\n",
        "        api_key = \"AIzaSyBXSKEf_KfmdYHvrLugt-KoJBSZyYXLG5A\"  # Replace with your actual API key\n",
        "        comments = get_video_comments(video_id, api_key)\n",
        "\n",
        "        if comments:\n",
        "            df = pd.DataFrame(comments)\n",
        "            df['published_at'] = pd.to_datetime(df['published_at'])  # Convert published_at to datetime\n",
        "            print(\"Comments retrieved successfully.\")\n",
        "            print(df.head())  # Display the first few rows of the DataFrame\n",
        "        else:\n",
        "            print(\"No comments retrieved.\")\n",
        "    else:\n",
        "        print(\"Invalid YouTube video URL. Please enter a valid URL.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "9ByuXiG-yegF",
        "outputId": "6b537428-433b-4d73-b423-8823f44ebc97"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'milvus'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ebf4f039e382>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmilvus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMilvus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'milvus'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install annoy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb_st6XH2KB-",
        "outputId": "9aca03a7-2e2c-4355-8a45-8941474cee65"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting annoy\n",
            "  Downloading annoy-1.17.3.tar.gz (647 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/647.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/647.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.1/647.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m645.1/647.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.3-cp310-cp310-linux_x86_64.whl size=552448 sha256=a17f973e1e0ada790c6afe7cfb2da9e26cac27457ffabd7c8589ea413b5d559a\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/8a/da/f714bcf46c5efdcfcac0559e63370c21abe961c48e3992465a\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import googleapiclient.discovery\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize  # Import normalize function\n",
        "from annoy import AnnoyIndex\n",
        "\n",
        "def extract_video_id(youtube_url):\n",
        "    \"\"\"\n",
        "    Extracts the video ID from a YouTube URL.\n",
        "\n",
        "    Args:\n",
        "        youtube_url (str): The YouTube video URL.\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted video ID.\n",
        "    \"\"\"\n",
        "    pattern = r\"(?<=v=)[a-zA-Z0-9_-]+(?=&|\\s|$)\"\n",
        "    match = re.search(pattern, youtube_url)\n",
        "    if match:\n",
        "        return match.group(0)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def get_video_comments(video_id, api_key):\n",
        "    \"\"\"\n",
        "    Retrieves comments from a YouTube video using the YouTube Data API v3.\n",
        "\n",
        "    Args:\n",
        "        video_id (str): The ID of the YouTube video.\n",
        "        api_key (str): The API key for accessing the YouTube Data API.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries containing comments data.\n",
        "    \"\"\"\n",
        "    youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=api_key)\n",
        "    comments = []\n",
        "    nextPageToken = None\n",
        "\n",
        "    while True:\n",
        "        request = youtube.commentThreads().list(\n",
        "            part=\"snippet\",\n",
        "            videoId=video_id,\n",
        "            maxResults=100,\n",
        "            pageToken=nextPageToken if nextPageToken else None\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            response = request.execute()\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            break\n",
        "\n",
        "        for item in response['items']:\n",
        "            comment = item['snippet']['topLevelComment']['snippet']\n",
        "            public = item['snippet']['isPublic']\n",
        "            comments.append({\n",
        "                \"author\": comment['authorDisplayName'],\n",
        "                \"published_at\": comment['publishedAt'],\n",
        "                \"like_count\": comment.get('likeCount', 0),  # Handle missing likeCount\n",
        "                \"text\": comment['textOriginal'],\n",
        "                \"public\": public\n",
        "            })\n",
        "\n",
        "        if 'nextPageToken' in response:\n",
        "            nextPageToken = response['nextPageToken']\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return comments\n",
        "\n",
        "def create_annoy_index(embeddings, dimension):\n",
        "    annoy_index = AnnoyIndex(dimension, 'angular')  # 'angular' similarity is suitable for cosine similarity\n",
        "    for i, embedding in enumerate(embeddings):\n",
        "        annoy_index.add_item(i, embedding)\n",
        "    annoy_index.build(10)  # 10 trees\n",
        "    return annoy_index\n",
        "\n",
        "def insert_vectors_annoy(comments, dimension):\n",
        "    # Vectorize comments using TF-IDF with specified dimensionality\n",
        "    text_corpus = [comment['text'] for comment in comments]\n",
        "    vectorizer = TfidfVectorizer(max_features=dimension)  # Explicitly specify dimensionality\n",
        "    tfidf_matrix = vectorizer.fit_transform(text_corpus)\n",
        "\n",
        "    # Normalize TF-IDF matrix\n",
        "    tfidf_matrix_normalized = normalize(tfidf_matrix)\n",
        "\n",
        "    # Convert TF-IDF matrix to dense array\n",
        "    tfidf_matrix_dense = tfidf_matrix_normalized.toarray()\n",
        "\n",
        "    # Create Annoy index\n",
        "    annoy_index = create_annoy_index(tfidf_matrix_dense, dimension)\n",
        "\n",
        "    # Save Annoy index to disk\n",
        "    annoy_index.save('annoy_index.ann')\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    youtube_url = input(\"Enter the YouTube video URL: \")\n",
        "    video_id = extract_video_id(youtube_url)\n",
        "\n",
        "    if video_id:\n",
        "        api_key = \"AIzaSyBXSKEf_KfmdYHvrLugt-KoJBSZyYXLG5A\"  # Replace with your actual API key\n",
        "        comments = get_video_comments(video_id, api_key)\n",
        "\n",
        "        if comments:\n",
        "            df = pd.DataFrame(comments)\n",
        "            df['published_at'] = pd.to_datetime(df['published_at'])  # Convert published_at to datetime\n",
        "            print(\"Comments retrieved successfully.\")\n",
        "            print(df.head())  # Display the first few rows of the DataFrame\n",
        "\n",
        "            # Insert vectorized comments into Annoy index\n",
        "            dimension = 100  # Dimensionality of the embedding vectors\n",
        "            insert_vectors_annoy(comments, dimension)\n",
        "            print(\"Comments inserted into Annoy index.\")\n",
        "        else:\n",
        "            print(\"No comments retrieved.\")\n",
        "    else:\n",
        "        print(\"Invalid YouTube video URL. Please enter a valid URL.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JueoOIux1L8k",
        "outputId": "8c6a5135-1559-4099-ada3-105d10d8085d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the YouTube video URL: https://www.youtube.com/watch?v=4BK1NycsNwQ\n",
            "Comments retrieved successfully.\n",
            "                      author              published_at  like_count  \\\n",
            "0  @ScalerSchoolOfTechnology 2023-06-16 06:24:32+00:00           5   \n",
            "1         @Dineshkumar-dh1bv 2024-05-24 09:04:00+00:00           0   \n",
            "2          @MohanKumar-or8eh 2024-05-10 18:05:50+00:00           0   \n",
            "3                @harrysmani 2024-05-05 16:53:18+00:00           1   \n",
            "4               @levi._.1709 2024-05-01 18:14:23+00:00           1   \n",
            "\n",
            "                                                text  public  \n",
            "0  Hi all, check out our video about SST's 3D cam...    True  \n",
            "1  Since when stanza living PG Lisbon changed int...    True  \n",
            "2  Hello sir  is there a micro campus  available ...    True  \n",
            "3  Replacing colleges with office buildings and h...    True  \n",
            "4  I just wanna ask something pls do reply this. ...    True  \n",
            "Comments inserted into Annoy index.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import googleapiclient.discovery\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from annoy import AnnoyIndex\n",
        "\n",
        "def extract_video_id(youtube_url):\n",
        "    \"\"\"\n",
        "    Extracts the video ID from a YouTube URL.\n",
        "\n",
        "    Args:\n",
        "        youtube_url (str): The YouTube video URL.\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted video ID.\n",
        "    \"\"\"\n",
        "    pattern = r\"(?<=v=)[a-zA-Z0-9_-]+(?=&|\\s|$)\"\n",
        "    match = re.search(pattern, youtube_url)\n",
        "    if match:\n",
        "        return match.group(0)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def get_video_comments(video_id, api_key):\n",
        "    \"\"\"\n",
        "    Retrieves comments from a YouTube video using the YouTube Data API v3.\n",
        "\n",
        "    Args:\n",
        "        video_id (str): The ID of the YouTube video.\n",
        "        api_key (str): The API key for accessing the YouTube Data API.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries containing comments data.\n",
        "    \"\"\"\n",
        "    youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=api_key)\n",
        "    comments = []\n",
        "    nextPageToken = None\n",
        "\n",
        "    while True:\n",
        "        request = youtube.commentThreads().list(\n",
        "            part=\"snippet\",\n",
        "            videoId=video_id,\n",
        "            maxResults=100,\n",
        "            pageToken=nextPageToken if nextPageToken else None\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            response = request.execute()\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            break\n",
        "\n",
        "        for item in response['items']:\n",
        "            comment = item['snippet']['topLevelComment']['snippet']\n",
        "            public = item['snippet']['isPublic']\n",
        "            comments.append({\n",
        "                \"author\": comment['authorDisplayName'],\n",
        "                \"published_at\": comment['publishedAt'],\n",
        "                \"like_count\": comment.get('likeCount', 0),  # Handle missing likeCount\n",
        "                \"text\": comment['textOriginal'],\n",
        "                \"public\": public\n",
        "            })\n",
        "\n",
        "        if 'nextPageToken' in response:\n",
        "            nextPageToken = response['nextPageToken']\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return comments\n",
        "\n",
        "def load_annoy_index(file_path):\n",
        "    annoy_index = AnnoyIndex(100, 'angular')  # Assuming dimensionality is 100\n",
        "    annoy_index.load(file_path)\n",
        "    return annoy_index\n",
        "\n",
        "def find_similar_comments(query, annoy_index, vectorizer, comments):\n",
        "    # Vectorize the query using the same vectorizer used for comments\n",
        "    query_vector = vectorizer.transform([query]).toarray()[0]\n",
        "    # Retrieve indices of similar comments from the Annoy index\n",
        "    similar_indices = annoy_index.get_nns_by_vector(query_vector, n=5)  # Retrieve top 5 most similar comments\n",
        "    # Get the actual comments corresponding to the indices\n",
        "    similar_comments = [comments[i] for i in similar_indices]\n",
        "    return similar_comments\n",
        "\n",
        "def main():\n",
        "    youtube_url = input(\"Enter the YouTube video URL: \")\n",
        "    video_id = extract_video_id(youtube_url)\n",
        "\n",
        "    if video_id:\n",
        "        api_key = \"AIzaSyBXSKEf_KfmdYHvrLugt-KoJBSZyYXLG5A\"  # Replace with your actual API key\n",
        "        comments = get_video_comments(video_id, api_key)\n",
        "\n",
        "        if comments:\n",
        "            df = pd.DataFrame(comments)\n",
        "            df['published_at'] = pd.to_datetime(df['published_at'])  # Convert published_at to datetime\n",
        "            print(\"Comments retrieved successfully.\")\n",
        "            print(df.head())  # Display the first few rows of the DataFrame\n",
        "\n",
        "            # Load Annoy index\n",
        "            annoy_index = load_annoy_index('annoy_index.ann')\n",
        "\n",
        "            # Vectorize comments using TF-IDF\n",
        "            text_corpus = df['text']\n",
        "            vectorizer = TfidfVectorizer(max_features=100)  # Assuming dimensionality is 100\n",
        "            tfidf_matrix = vectorizer.fit_transform(text_corpus)\n",
        "\n",
        "            query = input(\"Enter your query: \")\n",
        "            similar_comments = find_similar_comments(query, annoy_index, vectorizer, comments)\n",
        "            print(\"Top 5 most relevant comments to the query:\")\n",
        "            for idx, comment in enumerate(similar_comments, 1):\n",
        "                print(f\"Comment {idx}: {comment['text']}\")\n",
        "\n",
        "        else:\n",
        "            print(\"No comments retrieved.\")\n",
        "    else:\n",
        "        print(\"Invalid YouTube video URL. Please enter a valid URL.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBepLuZN27TS",
        "outputId": "0fc28c72-e39d-43cb-d2d4-b68cedfaa537"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the YouTube video URL: https://www.youtube.com/watch?v=4BK1NycsNwQ\n",
            "Comments retrieved successfully.\n",
            "                      author              published_at  like_count  \\\n",
            "0  @ScalerSchoolOfTechnology 2023-06-16 06:24:32+00:00           5   \n",
            "1         @Dineshkumar-dh1bv 2024-05-24 09:04:00+00:00           0   \n",
            "2          @MohanKumar-or8eh 2024-05-10 18:05:50+00:00           0   \n",
            "3                @harrysmani 2024-05-05 16:53:18+00:00           1   \n",
            "4               @levi._.1709 2024-05-01 18:14:23+00:00           1   \n",
            "\n",
            "                                                text  public  \n",
            "0  Hi all, check out our video about SST's 3D cam...    True  \n",
            "1  Since when stanza living PG Lisbon changed int...    True  \n",
            "2  Hello sir  is there a micro campus  available ...    True  \n",
            "3  Replacing colleges with office buildings and h...    True  \n",
            "4  I just wanna ask something pls do reply this. ...    True  \n",
            "Enter your query: office Buildings\n",
            "Top 5 most relevant comments to the query:\n",
            "Comment 1: Hi all, check out our video about SST's 3D campus tour and get a glimpse of life at our campus.\n",
            "https://www.youtube.com/watch?v=tHrXRP3-2HQ\n",
            "\n",
            "✅ Admissions ongoing for 2024.\n",
            "Learn more: https://bit.ly/SST_Website\n",
            "Comment 2: Since when stanza living PG Lisbon changed into micro house lol 😂😆 good.joke guys\n",
            "Comment 3: Hello sir  is there a micro campus  available in AC facilities please tell me sir\n",
            "Comment 4: Replacing colleges with office buildings and hostels with hotels. New age of college educations it seems. Just working on AI, NLP is not going to help the students. Need to have a strong base on wide range of subjects and then focussing on the area of specialization. No place to play and roam around for children, Something is lacking here. \n",
            "\n",
            "Is your college degree accrediated by NAAC and AICTE, how about students who wanted to go for higher studies like MBA, IAS etc.\n",
            "Comment 5: I just wanna ask something pls do reply this. \n",
            "\n",
            "As scaler is providing a bsc degree of bits pilani right. And according to me bsc isn't that worth tbh..\n",
            "\n",
            "And the main ques is, the course they will be teaching us at scaler is the same that they teach in btech cse programme as like iits, nits am talking abt the whole course, syllabus, etc.. Or they will be just teaching bsc? I just need to know that at scaler what do they ACTUALLY teach?? Is that similar/exact to btech in cs in iits and nits or something different??\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mc2SocQO4wsL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}